# Spiking Neural Networks (SNNs): Intuition & Mathematics

## The Core Philosophy

In traditional deep learning, information is **static** (snapshots).  
In **Spiking Neural Networks (SNNs)**, information is **temporal** (movies).

> **Time is not just a dimension — it is the data.**

---

## 1. The Paradigm Shift: ANN vs. SNN

To understand SNNs, we must unlearn how we view “numbers” in neural networks.

| Feature | Artificial Neural Networks (ANN) | Spiking Neural Networks (SNN) |
|------|----------------------------------|--------------------------------|
| Data Type | Continuous (float) | Binary (events) |
| Communication | Value magnitude | Spikes (0 or 1) |
| Dimension | Spatial (layers) | Spatio-temporal (layers + time) |
| Analogy | Photograph | Morse code |

### The Golden Rule

In an SNN, a neuron does **not** communicate magnitude.  
It communicates **time**.

> **“NOW!”**

Information is encoded in:
- Frequency (how often spikes occur)
- Latency (how early spikes occur)
- Synchronization (temporal alignment)

---

## 2. The Atomic Unit: The Spike

A spike is a binary event representing **an occurrence in time**.

Mathematically, a spike train is written as:

$$
S(t) = \sum_i \delta(t - t_i)
$$

Where \( t_i \) are spike times.

- Amplitude: irrelevant  
- Duration: instantaneous (ideal model)  
- Meaning: an event occurred  

---

## 3. The Neuron Model: Leaky Integrate-and-Fire (LIF)

The **Leaky Integrate-and-Fire (LIF)** neuron is the standard computational model used in SNNs.

### Intuition: Leaky Bucket

- Integrate: incoming spikes add voltage
- Leak: voltage decays over time
- Fire: voltage crosses threshold
- Reset: voltage is cleared

---

### Coincidence Detection

If spikes arrive **close in time**, voltage accumulates.  
If spikes are **spread out**, leakage dominates.

> The LIF neuron naturally detects **temporal coincidence**.

---

## 4. The Physics: Continuous-Time Model

The membrane potential \( V(t) \) follows:

$$
\tau \frac{dV(t)}{dt} = -(V(t) - V_{rest}) + R \cdot I(t)
$$

Where:
- \( V(t) \) — membrane potential
- \( V_{rest} \) — resting potential
- \( \tau \) — membrane time constant
- \( R \) — membrane resistance
- \( I(t) \) — input current

---

### Simplified Model

Assuming:
- \( V_{rest} = 0 \)
- \( R = 1 \)

$$
\tau \frac{dV(t)}{dt} = -V(t) + I(t)
$$

---

### Spike-Based Input Current

In spiking neurons, current is generated by incoming spikes:

$$
I(t) = \sum_i w_i s_i(t)
$$

Substituting:

$$
\tau \frac{dV(t)}{dt} = -V(t) + \sum_i w_i s_i(t)
$$

---

### Solution of the Differential Equation

Solving gives:

$$
V(t) = \sum_k w_k e^{-\frac{(t - t_k)}{\tau}}
$$

Each spike leaves an **exponentially decaying trace**.

> **Coincidence = overlapping exponentials**

---

## 5. Discrete-Time (Digital) Model

Digital systems operate in time steps \( \Delta t \).

Using Euler discretization:

$$
V[t+1] = \beta V[t] + \sum_i w_i x_i[t] - S_{out}[t] \cdot V_{th}
$$

Where:

$$
\beta = e^{-\frac{\Delta t}{\tau}} \approx 1 - \frac{\Delta t}{\tau}
$$

---

### Interpretation

- $\beta V[t]$ — memory of the past  
- $\sum_i w_i x_i[t]$ — present input  
- $S_{out}[t] \cdot V_{th}$ — reset after firing  

This balance enables **stable temporal computation**.


---

## 6. Discrete LIF Pseudocode

```python
mem = 0

for t in range(time_steps):
    # Leak
    mem = mem * beta

    # Integrate spikes
    mem = mem + input_spikes[t]

    # Threshold
    if mem >= threshold:
        spike = 1
        mem = 0
    else:
        spike = 0

    spike_train_out.append(spike)
```

>Final Insight

**Spiking Neural Networks compute by integrating recent events over time and firing when enough spikes align temporally.**